{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54d39f6b-f2f8-4678-b838-5a5c60be615d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting frontend\n",
      "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
      "Requirement already satisfied: starlette>=0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (0.45.3)\n",
      "Requirement already satisfied: uvicorn>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (0.34.0)\n",
      "Collecting itsdangerous>=1.1.0 (from frontend)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aiofiles (from frontend)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from starlette>=0.12.0->frontend) (4.8.0)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.1)\n",
      "Downloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: itsdangerous, aiofiles, frontend\n",
      "Successfully installed aiofiles-24.1.0 frontend-0.0.3 itsdangerous-2.2.0\n",
      "Requirement already satisfied: streamlit in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.42.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (2.1.4)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (4.23.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.28.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: tools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: pytils in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (0.4.1)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (1.17.0)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (5.3.1)\n",
      "Requirement already satisfied: optuna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.14.1)\n",
      "Requirement already satisfied: colorlog in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (2.0.38)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install frontend\n",
    "!pip install streamlit\n",
    "!pip install tools\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792819a8-ee96-4147-8eba-03c224df1e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting configobj (from fitz)\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting configparser (from fitz)\n",
      "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting httplib2 (from fitz)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nibabel (from fitz)\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting nipype (from fitz)\n",
      "  Downloading nipype-1.9.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (2.1.4)\n",
      "Collecting pyxnat (from fitz)\n",
      "  Downloading pyxnat-1.6.3-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.11.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httplib2->fitz) (3.2.1)\n",
      "Collecting importlib-resources>=5.12 (from nibabel->fitz)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging>=20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nibabel->fitz) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nibabel->fitz) (4.12.2)\n",
      "Requirement already satisfied: click>=6.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (8.1.8)\n",
      "Requirement already satisfied: networkx>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.4.2)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pydot>=1.2.3 (from nipype->fitz)\n",
      "  Downloading pydot-3.0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
      "  Downloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting traits>=6.2 (from nipype->fitz)\n",
      "  Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.17.0)\n",
      "Collecting acres (from nipype->fitz)\n",
      "  Downloading acres-0.3.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting etelemetry>=0.3.1 (from nipype->fitz)\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion!=1.2 (from nipype->fitz)\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting puremagic (from nipype->fitz)\n",
      "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->fitz) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->fitz) (2025.1)\n",
      "Requirement already satisfied: lxml>=4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyxnat->fitz) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Collecting pathlib>=1.0 (from pyxnat->fitz)\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2025.1.31)\n",
      "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Downloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nipype-1.9.2-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyxnat-1.6.3-py3-none-any.whl (95 kB)\n",
      "Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "Downloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
      "Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simplejson-3.20.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading acres-0.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
      "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Installing collected packages: puremagic, pathlib, looseversion, traits, simplejson, pydot, isodate, importlib-resources, httplib2, configparser, configobj, ci-info, rdflib, pyxnat, nibabel, etelemetry, acres, prov, nipype, fitz\n",
      "Successfully installed acres-0.3.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 httplib2-0.22.0 importlib-resources-6.5.2 isodate-0.6.1 looseversion-1.3.0 nibabel-5.3.2 nipype-1.9.2 pathlib-1.0.1 prov-2.0.1 puremagic-1.28 pydot-3.0.4 pyxnat-1.6.3 rdflib-6.3.2 simplejson-3.20.1 traits-7.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b942efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39d380fbd054a63a6fbef3a07ab2edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 07:48:06,086] A new study created in memory with name: no-name-1d279568-574f-41dc-9e4f-9dbd3f5774be\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.196242444289175` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7888446935379861` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `48` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:48:37,437] Trial 0 finished with value: 1102.0 and parameters: {'max_new_tokens': 264, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.196242444289175, 'top_p': 0.7888446935379861, 'top_k': 48}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.11673044047887313` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7673773298299861` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `44` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:49:09,800] Trial 1 finished with value: 1048.0 and parameters: {'max_new_tokens': 277, 'no_repeat_ngram_size': 4, 'num_beams': 5, 'temperature': 0.11673044047887313, 'top_p': 0.7673773298299861, 'top_k': 44}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.23575516968631188` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7737967504455827` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `58` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:49:36,614] Trial 2 finished with value: 907.0 and parameters: {'max_new_tokens': 214, 'no_repeat_ngram_size': 4, 'num_beams': 6, 'temperature': 0.23575516968631188, 'top_p': 0.7737967504455827, 'top_k': 58}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.12356185572684573` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8337565641208515` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `43` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:50:07,756] Trial 3 finished with value: 1102.0 and parameters: {'max_new_tokens': 268, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.12356185572684573, 'top_p': 0.8337565641208515, 'top_k': 43}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.23080492433077918` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7956369763649193` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `45` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:50:34,139] Trial 4 finished with value: 1102.0 and parameters: {'max_new_tokens': 226, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.23080492433077918, 'top_p': 0.7956369763649193, 'top_k': 45}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.13142754301751067` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7508100463765599` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `54` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:50:57,831] Trial 5 finished with value: 1102.0 and parameters: {'max_new_tokens': 203, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.13142754301751067, 'top_p': 0.7508100463765599, 'top_k': 54}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.17501799411553037` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7689833041912001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `60` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:51:25,740] Trial 6 finished with value: 1041.0 and parameters: {'max_new_tokens': 222, 'no_repeat_ngram_size': 3, 'num_beams': 6, 'temperature': 0.17501799411553037, 'top_p': 0.7689833041912001, 'top_k': 60}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.23171237060418698` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7505349308005845` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:51:55,826] Trial 7 finished with value: 1041.0 and parameters: {'max_new_tokens': 238, 'no_repeat_ngram_size': 3, 'num_beams': 6, 'temperature': 0.23171237060418698, 'top_p': 0.7505349308005845, 'top_k': 54}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.15645755411113227` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8054363604451208` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:52:26,997] Trial 8 finished with value: 1102.0 and parameters: {'max_new_tokens': 270, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.15645755411113227, 'top_p': 0.8054363604451208, 'top_k': 48}. Best is trial 0 with value: 1102.0.\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.19533770523569235` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8193528473997697` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "[I 2025-03-02 07:52:55,371] Trial 9 finished with value: 907.0 and parameters: {'max_new_tokens': 223, 'no_repeat_ngram_size': 4, 'num_beams': 6, 'temperature': 0.19533770523569235, 'top_p': 0.8193528473997697, 'top_k': 48}. Best is trial 0 with value: 1102.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters: {'max_new_tokens': 264, 'no_repeat_ngram_size': 3, 'num_beams': 5, 'temperature': 0.196242444289175, 'top_p': 0.7888446935379861, 'top_k': 48}\n",
      "\n",
      "Summary for Criminal_Case_File_John_Doe.pdf:\n",
      "The defendant was charged with first-degree murder, aggravated assault, and burglary in connection with the death of Emily White. The prosecution argued that the defendant had a motive to kill the victim and that he had the opportunity to commit the crime. The defense argued that there was no evidence linking the defendant to the crime and that the prosecution had failed to prove its case beyond a reasonable doubt. The court found that the evidence was sufficient to support a finding of guilt. The State is seeking a sentence of life in prison without the possibility of parole for the defendant, John Doe. The defendant is charged with one count of murder in the first degree, one count conspiracy to commit murder, and one count possession of a firearm by a person who has previously been adjudicated delinquent for a violent felony. The prosecution intends to prove that the defendant is guilty beyond a reasonable doubt of all three charges. The defense will argue that there is not enough evidence to convict the defendant of any of the charges and that he should be acquitted of all of them.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#ACCEPTANCE 1\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import optuna\n",
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF for handling PDF files\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes illegal characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def chunk_text(text, max_length=2000):\n",
    "    \"\"\"Splits text into manageable chunks while preserving full sentences.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if len(current_chunk + \" \" + word) <= max_length:\n",
    "            current_chunk += \" \" + word if current_chunk else word\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ========== PDF TEXT EXTRACTION FUNCTION ==========\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTION ==========\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads model and tokenizer efficiently.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ========== ADVANCED LEGAL PROMPT GENERATOR ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates an optimized legal case summarization prompt based on case type.\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"contract_dispute\": (\n",
    "            \"Summarize the following legal case with a focus on contractual obligations, \"\n",
    "            \"breach details, and the legal consequences of non-compliance. \"\n",
    "            \"Ensure that arbitration or court rulings are accurately represented. \"\n",
    "            \"Explicitly outline whether force majeure was invoked and whether it was legally upheld. \"\n",
    "            \"Avoid unnecessary commentary, fabricated case numbers, or unrelated legal precedents.\"\n",
    "        ),\n",
    "        \"employment_law\": (\n",
    "            \"Provide a concise and structured summary of the employment dispute, \"\n",
    "            \"highlighting claims by both employer and employee, key evidence presented, \"\n",
    "            \"and the final ruling. Clearly state if labor laws, wrongful termination, \"\n",
    "            \"or workplace discrimination played a role in the judgment. \"\n",
    "            \"Avoid unrelated legal references or fabricated citations.\"\n",
    "        ),\n",
    "        \"criminal_law\": (\n",
    "            \"Summarize the criminal case by identifying the charges, legal arguments from both prosecution and defense, \"\n",
    "            \"and the final court ruling. Clearly differentiate between factual evidence, witness testimonies, \"\n",
    "            \"and applicable laws referenced during the proceedings. \"\n",
    "            \"Do not fabricate legal citations or case numbers.\"\n",
    "        ),\n",
    "        \"intellectual_property\": (\n",
    "            \"Summarize the legal case focusing on intellectual property rights, infringement claims, \"\n",
    "            \"and legal reasoning behind the court’s decision. Explicitly mention whether fair use, \"\n",
    "            \"patent validity, or copyright law principles were applied. \"\n",
    "            \"Ensure clarity without adding fabricated legal precedents or citations.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Default fallback prompt for unlisted case types\n",
    "    default_prompt = (\n",
    "        \"Summarize the legal case with a structured approach, ensuring clarity, factual accuracy, \"\n",
    "        \"and legal precision. Identify key issues, claims from both parties, and the final ruling. \"\n",
    "        \"Ensure the summary remains strictly within the context of the case without adding fabricated case numbers, \"\n",
    "        \"unrelated legal precedents, or speculative conclusions.\"\n",
    "    )\n",
    "\n",
    "    return prompts.get(case_type.lower(), default_prompt)\n",
    "\n",
    "# ========== SUMMARY CLEANING FUNCTION ==========\n",
    "\n",
    "def clean_summary(summary):\n",
    "    \"\"\"Ensures the generated summary ends naturally without unnecessary repetitions.\"\"\"\n",
    "    summary = re.sub(r'### End-Note:.*', '', summary, flags=re.DOTALL).strip()\n",
    "    summary = re.sub(r'### Response:.*', '', summary, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Limit excessive conclusions\n",
    "    sentences = summary.split(\". \")\n",
    "    if len(sentences) > 4:  # Limit the length of the summary\n",
    "        summary = \". \".join(sentences[:4]) + \".\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTION ==========\n",
    "\n",
    "def generate_summary(input_text, model, tokenizer, trial, case_type):\n",
    "    \"\"\"Generates a structured summary with optimized hyperparameters.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\"\n",
    "\n",
    "    # Dynamic Hyperparameter Tuning\n",
    "    max_new_tokens = trial.suggest_int(\"max_new_tokens\", 200, 300)\n",
    "    no_repeat_ngram_size = trial.suggest_int(\"no_repeat_ngram_size\", 3, 4)\n",
    "    num_beams = trial.suggest_int(\"num_beams\", 5, 6)\n",
    "    temperature = trial.suggest_float(\"temperature\", 0.1, 0.25)\n",
    "    top_p = trial.suggest_float(\"top_p\", 0.75, 0.85)\n",
    "    top_k = trial.suggest_int(\"top_k\", 40, 60)\n",
    "    do_sample = False  # Force deterministic output\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        input_str,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    summary_output = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        do_sample=do_sample\n",
    "    )\n",
    "\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    marker = \"### Summary:\"\n",
    "    summary = full_output.split(marker, 1)[1].strip() if marker in full_output else full_output.strip()\n",
    "\n",
    "    return sanitize_text(clean_summary(summary))\n",
    "\n",
    "# ========== PDF PROCESSING FUNCTION ==========\n",
    "\n",
    "def process_pdf_folder(folder_path, case_type, model, tokenizer, trial):\n",
    "    \"\"\"Process all PDF files in a folder and generate summaries.\"\"\"\n",
    "    results = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "            sanitized_text = sanitize_text(text)\n",
    "            chunks = chunk_text(sanitized_text)\n",
    "            summaries = [generate_summary(chunk, model, tokenizer, trial, case_type) for chunk in chunks]\n",
    "            final_summary = \" \".join(summaries)\n",
    "            results[filename] = final_summary\n",
    "    return results\n",
    "\n",
    "# ========== OPTUNA OBJECTIVE FUNCTION ==========\n",
    "\n",
    "def objective(trial, folder_path, case_type, model, tokenizer):\n",
    "    \"\"\"Objective function for hyperparameter tuning with Optuna.\"\"\"\n",
    "    results = process_pdf_folder(folder_path, case_type, model, tokenizer, trial)\n",
    "    total_length = sum(len(summary) for summary in results.values())\n",
    "    return total_length  # You can modify this to use a more meaningful metric\n",
    "\n",
    "# ========== MAIN SCRIPT EXECUTION ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model and tokenizer\n",
    "    model_name = \"coderop12/Empowering_Legal_Summarization\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Specify the folder containing PDF files and the case type\n",
    "    folder_path = input(\"Enter the path to the directory containing the PDF files: \")\n",
    "    case_type = input(\"Enter the case type (e.g., contract_dispute, employment_law, criminal_law, intellectual_property): \")\n",
    "\n",
    "    # Optimize parameters using Optuna\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, folder_path, case_type, model, tokenizer), n_trials=10)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_params = best_trial.params\n",
    "    print(f\"\\nBest Hyperparameters: {best_params}\\n\")\n",
    "\n",
    "    # Process the PDF folder and generate summaries using the best parameters\n",
    "    results = process_pdf_folder(folder_path, case_type, model, tokenizer, best_trial)\n",
    "\n",
    "    # Print the summaries\n",
    "    for file, summary in results.items():\n",
    "        print(f\"Summary for {file}:\")\n",
    "        print(summary)\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7aba2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FPDF\n",
      "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: FPDF\n",
      "  Building wheel for FPDF (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for FPDF: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40758 sha256=fcee241ff767c723af9965305c3bc89d300fa1772b8536485f2fc268038b60a9\n",
      "  Stored in directory: /home/zeus/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
      "Successfully built FPDF\n",
      "Installing collected packages: FPDF\n",
      "Successfully installed FPDF-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2d99b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95fa17574174810820e9122926615d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 08:37:45,705] A new study created in memory with name: no-name-f3de6887-3ff1-40c8-8c3f-aeeec486afdd\n",
      "[I 2025-03-02 08:38:11,361] Trial 0 finished with value: 0.4417839010366038 and parameters: {'max_new_tokens': 343, 'num_beams': 5, 'temperature': 0.1837575119456632, 'do_sample': True, 'top_p': 0.8518569772873625}. Best is trial 0 with value: 0.4417839010366038.\n",
      "[I 2025-03-02 08:38:30,874] Trial 1 finished with value: 0.4018971765938979 and parameters: {'max_new_tokens': 304, 'num_beams': 4, 'temperature': 0.19608899091576665, 'do_sample': True, 'top_p': 0.9169928763336238}. Best is trial 0 with value: 0.4417839010366038.\n",
      "[I 2025-03-02 08:38:50,459] Trial 2 finished with value: 0.39706987248700076 and parameters: {'max_new_tokens': 303, 'num_beams': 4, 'temperature': 0.1339902249714613, 'do_sample': True, 'top_p': 0.9466764884383407}. Best is trial 0 with value: 0.4417839010366038.\n",
      "[I 2025-03-02 08:39:15,147] Trial 3 finished with value: 0.44186194967225995 and parameters: {'max_new_tokens': 345, 'num_beams': 5, 'temperature': 0.21314389234480785, 'do_sample': True, 'top_p': 0.8504364047421918}. Best is trial 3 with value: 0.44186194967225995.\n",
      "[I 2025-03-02 08:39:37,647] Trial 4 finished with value: 0.4238829432740788 and parameters: {'max_new_tokens': 311, 'num_beams': 5, 'temperature': 0.216581593775798, 'do_sample': True, 'top_p': 0.9331402215472874}. Best is trial 3 with value: 0.44186194967225995.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL SUMMARY ===\n",
      "\n",
      "John Doe is charged with first-degree murder, aggravated assault, and burglary in connection with the death of Emily White, a high school teacher in Metropolis. The prosecution intends to prove that John Doe killed Emily White by stabbing her multiple times in her own apartment. The defense will argue that there is no evidence linking John Doe to the crime and that he is innocent of all charges. The facts of the case are as follows: On the night of the incident, the victim was found dead by her neighbor. The police were called to the scene and found that the victim had been stabbed multiple times. The victim was taken to the local hospital, where she was pronounced dead on arrival. The investigation revealed that the defendant had been in a relationship with Emily White for several months prior to the incident. The two had a tumultuous relationship and had argued on several occasions. On the day of the crime, the defendant called the victim and asked her to meet him at his apartment. When the victim arrived at the apartment, she was met by the defendant, who was armed with a knife. He stabbed her several times in the chest and back. She was able to escape the apartment and ran to her neighbor’s house for help. The neighbor called the police and the police arrived to find the victim lying on the floor of her apartment. They found the defendant running away from the apartment. He was arrested and charged with the crime. The evidence in the case includes: 1. DNA evidence: DNA evidence was collected from the crime scene and was matched to John Doe. 2. Fingerprint evidence: Fingerprint evidence was also collected and was also matched to him. 3. Security footage: Security footage from a nearby building showed John Doe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import optuna\n",
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes illegal characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def chunk_text(text, max_length=3000):\n",
    "    \"\"\"Splits long text into smaller, structured chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) + 1 <= max_length:\n",
    "            current_chunk += \" \" + word if current_chunk else word\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ========== PDF TEXT EXTRACTION FUNCTION ==========\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts and processes text from a PDF file.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' does not exist.\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        doc.close()\n",
    "        return sanitize_text(text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTION ==========\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer and model efficiently.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ========== ADVANCED LEGAL PROMPT GENERATOR ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates structured prompts with few-shot examples for better accuracy.\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"criminal_law\": \"\"\"\n",
    "You are an expert legal assistant summarizing criminal cases.\n",
    "Create a detailed summary with the following structure:\n",
    "1. Defendant details and charges\n",
    "2. Evidence collected\n",
    "3. Key prosecution arguments\n",
    "4. Key defense arguments\n",
    "5. Current case status\n",
    "\n",
    "Focus on key elements that determine the case outcome.\n",
    "\"\"\",\n",
    "        \"contract_dispute\": \"\"\"\n",
    "You are an expert legal assistant summarizing contract disputes.\n",
    "Create a detailed summary with the following structure:\n",
    "1. Parties involved\n",
    "2. Nature of the contract and alleged breach\n",
    "3. Key legal arguments from plaintiff\n",
    "4. Key legal arguments from defendant\n",
    "5. Case status or resolution\n",
    "\n",
    "Focus on key elements that determine the case outcome.\n",
    "\"\"\",\n",
    "        \"employment_law\": \"\"\"\n",
    "You are an expert legal assistant summarizing employment law cases.\n",
    "Create a structured summary with:\n",
    "1. Employee/employer details\n",
    "2. Nature of the dispute\n",
    "3. Key claims by employee\n",
    "4. Key defense by employer\n",
    "5. Case status or ruling\n",
    "\n",
    "Focus on key elements that determine the case outcome.\n",
    "\"\"\",\n",
    "        \"intellectual_property\": \"\"\"\n",
    "You are an expert legal assistant summarizing intellectual property cases.\n",
    "Create a structured summary with:\n",
    "1. Parties involved\n",
    "2. Nature of alleged infringement\n",
    "3. Key arguments from plaintiff\n",
    "4. Key arguments from defendant\n",
    "5. Case status or ruling\n",
    "\n",
    "Focus on key elements that determine the case outcome.\n",
    "\"\"\",\n",
    "        \"default_prompt\": \"\"\"\n",
    "Summarize the legal case with clarity:\n",
    "1. Key parties involved\n",
    "2. Nature of the dispute\n",
    "3. Main arguments from both sides\n",
    "4. Current status or resolution\n",
    "\n",
    "Focus on key elements that determine the case outcome.\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    return prompts.get(case_type.lower(), prompts[\"default_prompt\"])\n",
    "\n",
    "# ========== EVALUATION FUNCTION ==========\n",
    "\n",
    "def evaluate_summary(generated_summary, reference_text):\n",
    "    \"\"\"\n",
    "    Evaluates the quality of the generated summary.\n",
    "    Returns a score that can be used for optimization.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text[:1000], generated_summary)\n",
    "    \n",
    "    combined_score = (scores['rouge1'].fmeasure + scores['rouge2'].fmeasure + scores['rougeL'].fmeasure) / 3.0\n",
    "    \n",
    "    # Additional heuristics for summary quality\n",
    "    words = generated_summary.split()\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    # 1. Prefer summaries of reasonable length\n",
    "    length_score = min(1.0, len(words) / 350)\n",
    "    \n",
    "    # 2. Penalize repetitions\n",
    "    repetition_score = len(unique_words) / max(1, len(words))\n",
    "    \n",
    "    # 3. Reward information density\n",
    "    info_density = min(1.0, len(unique_words) / 200)\n",
    "    \n",
    "    # Final weighted score\n",
    "    final_score = (combined_score * 0.5) + (length_score * 0.2) + (repetition_score * 0.2) + (info_density * 0.1)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTION ==========\n",
    "\n",
    "def generate_summary(input_text, model, tokenizer, hyperparams, case_type):\n",
    "    \"\"\"Generates structured summary with specified hyperparameters.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\\n\"\n",
    "\n",
    "    model_inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=hyperparams.get(\"max_new_tokens\", 300),\n",
    "            num_beams=hyperparams.get(\"num_beams\", 4),\n",
    "            temperature=hyperparams.get(\"temperature\", 0.2),\n",
    "            do_sample=hyperparams.get(\"do_sample\", True),\n",
    "            top_p=hyperparams.get(\"top_p\", 0.9),\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return sanitize_text(full_output.split(\"### Summary:\")[-1].strip())\n",
    "\n",
    "# ========== OBJECTIVE FUNCTION FOR OPTUNA ==========\n",
    "\n",
    "def objective(trial, input_text, model, tokenizer, case_type, reference_text):\n",
    "    \"\"\"Objective function for Optuna optimization.\"\"\"\n",
    "    summary = generate_summary(input_text, model, tokenizer, {\n",
    "        \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 250, 350),\n",
    "        \"num_beams\": trial.suggest_int(\"num_beams\", 3, 5),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 0.1, 0.3),\n",
    "        \"do_sample\": trial.suggest_categorical(\"do_sample\", [True]),\n",
    "        \"top_p\": trial.suggest_float(\"top_p\", 0.85, 0.95)\n",
    "    }, case_type)\n",
    "    \n",
    "    return evaluate_summary(summary, reference_text)\n",
    "\n",
    "# ========== MAIN EXECUTION BLOCK ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"coderop12/Empowering_Legal_Summarization\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    pdf_path = input(\"Enter the path to the PDF file: \")\n",
    "    input_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if input_text:\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(lambda trial: objective(trial, input_text, model, tokenizer, \"default_prompt\", input_text), n_trials=5)\n",
    "\n",
    "        print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "        print(generate_summary(input_text, model, tokenizer, study.best_trial.params, \"default_prompt\"))\n",
    "    else:\n",
    "        print(\"No text extracted from the PDF. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1af14e950b94f5e836f71e6b22ac4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 08:46:44,740] A new study created in memory with name: no-name-713185b6-e8a7-4ec7-b596-d2235d22a9b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing summarization parameters with 15 trials for conciseness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 08:46:57,788] Trial 0 finished with value: 970.0 and parameters: {'max_new_tokens': 188, 'num_beams': 5, 'temperature': 0.1548510009339974, 'do_sample': True, 'top_p': 0.8073313972911808}. Best is trial 0 with value: 970.0.\n",
      "[I 2025-03-02 08:47:08,955] Trial 1 finished with value: 836.0 and parameters: {'max_new_tokens': 158, 'num_beams': 5, 'temperature': 0.2718870897849227, 'do_sample': True, 'top_p': 0.8650483669703447}. Best is trial 0 with value: 970.0.\n",
      "[I 2025-03-02 08:47:21,370] Trial 2 finished with value: 823.0 and parameters: {'max_new_tokens': 158, 'num_beams': 6, 'temperature': 0.16931874656943355, 'do_sample': True, 'top_p': 0.8156332015587646}. Best is trial 0 with value: 970.0.\n",
      "[I 2025-03-02 08:47:36,257] Trial 3 finished with value: 978.0 and parameters: {'max_new_tokens': 190, 'num_beams': 6, 'temperature': 0.12826875231479046, 'do_sample': True, 'top_p': 0.9473973229061159}. Best is trial 3 with value: 978.0.\n",
      "[I 2025-03-02 08:47:48,986] Trial 4 finished with value: 1004.0 and parameters: {'max_new_tokens': 194, 'num_beams': 4, 'temperature': 0.1858260043675128, 'do_sample': True, 'top_p': 0.8895649707773327}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:47:56,285] Trial 5 finished with value: 559.0 and parameters: {'max_new_tokens': 103, 'num_beams': 5, 'temperature': 0.12579197054176533, 'do_sample': True, 'top_p': 0.905189860526903}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:48:04,767] Trial 6 finished with value: 670.0 and parameters: {'max_new_tokens': 127, 'num_beams': 4, 'temperature': 0.19330600984679355, 'do_sample': True, 'top_p': 0.9444982910582052}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:48:16,005] Trial 7 finished with value: 918.0 and parameters: {'max_new_tokens': 176, 'num_beams': 4, 'temperature': 0.1917972214038508, 'do_sample': True, 'top_p': 0.9093885516151732}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:48:28,094] Trial 8 finished with value: 962.0 and parameters: {'max_new_tokens': 185, 'num_beams': 4, 'temperature': 0.29502654804413186, 'do_sample': True, 'top_p': 0.8151674877015852}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:48:39,047] Trial 9 finished with value: 810.0 and parameters: {'max_new_tokens': 153, 'num_beams': 5, 'temperature': 0.11721489748606301, 'do_sample': True, 'top_p': 0.9423641408451748}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:48:47,522] Trial 10 finished with value: 690.0 and parameters: {'max_new_tokens': 131, 'num_beams': 4, 'temperature': 0.24220345531481463, 'do_sample': True, 'top_p': 0.8750266205331463}. Best is trial 4 with value: 1004.0.\n",
      "[I 2025-03-02 08:49:02,893] Trial 11 finished with value: 1011.0 and parameters: {'max_new_tokens': 196, 'num_beams': 6, 'temperature': 0.23276796488165646, 'do_sample': True, 'top_p': 0.9083700880458601}. Best is trial 11 with value: 1011.0.\n",
      "[I 2025-03-02 08:49:18,778] Trial 12 finished with value: 1029.0 and parameters: {'max_new_tokens': 200, 'num_beams': 6, 'temperature': 0.22585754861185142, 'do_sample': True, 'top_p': 0.8885578867702024}. Best is trial 12 with value: 1029.0.\n",
      "[I 2025-03-02 08:49:34,674] Trial 13 finished with value: 1029.0 and parameters: {'max_new_tokens': 200, 'num_beams': 6, 'temperature': 0.22999570859145765, 'do_sample': True, 'top_p': 0.8481464160843977}. Best is trial 12 with value: 1029.0.\n",
      "[I 2025-03-02 08:49:48,022] Trial 14 finished with value: 882.0 and parameters: {'max_new_tokens': 170, 'num_beams': 6, 'temperature': 0.22826363996395543, 'do_sample': True, 'top_p': 0.8448098817915835}. Best is trial 12 with value: 1029.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL CONCISE SUMMARY ===\n",
      "\n",
      "The defendant was charged with first-degree murder, aggravated assault, and burglary in connection with the death of Emily White. The prosecution argued that the defendant had a motive to kill the victim and that he had the opportunity to commit the crime. The defense argued that there was no evidence linking the defendant to the crime and that the prosecution had not proven its case beyond a reasonable doubt. The case is still pending and the defendant is presumed innocent until proven guilty in a court of law. The facts of the case are as follows: The defendant was a freelance graphic designer and the victim was a high school teacher. They had been dating for a few months before the incident. On the night of the incident, the defendant called the victim several times but she did not answer. He then went to her apartment and forced his way in. The victim was found dead in her bed with multiple stab wounds to her neck and chest. The police were called to the scene and they found the defendant hiding in the closet.\n"
     ]
    }
   ],
   "source": [
    "'''Parameter\tPrevious Tokens\tPresent Tokens\tImpact\n",
    "max_new_tokens\t250 - 350\t100 - 200\tShorter, more concise summaries\n",
    "num_beams\t3 - 5\t4 - 6\tHigher quality, fewer errors\n",
    "top_p\t0.75 - 0.85\t0.8 - 0.95\tBetter token selection for clarity\n",
    "do_sample\tFalse\tTrue\tMore diverse yet accurate text\n",
    "'''\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import optuna\n",
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes illegal characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts and processes text from a PDF file.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' does not exist.\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        doc.close()\n",
    "        return sanitize_text(text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTION ==========\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer and model efficiently.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ========== LEGAL PROMPT GENERATOR ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates structured prompts for concise summarization.\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"criminal_law\": \"\"\"\n",
    "Summarize this criminal case concisely:\n",
    "1. Defendant details and charges\n",
    "2. Key evidence presented\n",
    "3. Prosecution's main arguments\n",
    "4. Defense counterarguments\n",
    "5. Case status\n",
    "\"\"\",\n",
    "        \"contract_dispute\": \"\"\"\n",
    "Summarize this contract dispute case concisely:\n",
    "1. Parties involved\n",
    "2. Nature of the contract and alleged breach\n",
    "3. Key legal arguments from both sides\n",
    "4. Case status or resolution\n",
    "\"\"\",\n",
    "        \"employment_law\": \"\"\"\n",
    "Summarize this employment law case concisely:\n",
    "1. Employee/employer details\n",
    "2. Nature of the dispute\n",
    "3. Key claims by the employee\n",
    "4. Key defenses by the employer\n",
    "5. Case status\n",
    "\"\"\",\n",
    "        \"intellectual_property\": \"\"\"\n",
    "Summarize this intellectual property case concisely:\n",
    "1. Parties involved\n",
    "2. Nature of alleged infringement\n",
    "3. Key legal arguments from both sides\n",
    "4. Case status or ruling\n",
    "\"\"\",\n",
    "        \"default_prompt\": \"\"\"\n",
    "Summarize this legal case concisely:\n",
    "1. Key parties involved\n",
    "2. Nature of the dispute\n",
    "3. Main arguments from both sides\n",
    "4. Current status or resolution\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    return prompts.get(case_type.lower(), prompts[\"default_prompt\"])\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTION ==========\n",
    "\n",
    "def generate_concise_summary(input_text, model, tokenizer, hyperparams, case_type):\n",
    "    \"\"\"Generates a structured, concise summary with optimized hyperparameters.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\\n\"\n",
    "\n",
    "    model_inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=hyperparams.get(\"max_new_tokens\", 150),  # Enforced conciseness\n",
    "            num_beams=hyperparams.get(\"num_beams\", 5),  \n",
    "            temperature=hyperparams.get(\"temperature\", 0.2),\n",
    "            do_sample=hyperparams.get(\"do_sample\", True),\n",
    "            top_p=hyperparams.get(\"top_p\", 0.85),\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return sanitize_text(full_output.split(\"### Summary:\")[-1].strip())\n",
    "\n",
    "# ========== OBJECTIVE FUNCTION FOR OPTUNA ==========\n",
    "\n",
    "def objective(trial, input_text, model, tokenizer, case_type):\n",
    "    \"\"\"Objective function for hyperparameter tuning using Optuna.\"\"\"\n",
    "    summary = generate_concise_summary(input_text, model, tokenizer, {\n",
    "        \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 100, 200),  # Concise summary\n",
    "        \"num_beams\": trial.suggest_int(\"num_beams\", 4, 6),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 0.1, 0.3),\n",
    "        \"do_sample\": trial.suggest_categorical(\"do_sample\", [True]),\n",
    "        \"top_p\": trial.suggest_float(\"top_p\", 0.8, 0.95)\n",
    "    }, case_type)\n",
    "\n",
    "    return len(summary)  # Placeholder scoring (use evaluation metric if needed)\n",
    "\n",
    "# ========== MAIN EXECUTION BLOCK ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"coderop12/Empowering_Legal_Summarization\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    pdf_path = input(\"Enter the path to the PDF file: \")\n",
    "    input_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if input_text:\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        print(\"Optimizing summarization parameters with 15 trials for conciseness...\")\n",
    "        study.optimize(lambda trial: objective(trial, input_text, model, tokenizer, \"default_prompt\"), n_trials=15)  \n",
    "\n",
    "        print(\"\\n=== FINAL CONCISE SUMMARY ===\\n\")\n",
    "        concise_summary = generate_concise_summary(input_text, model, tokenizer, study.best_trial.params, \"default_prompt\")\n",
    "        print(concise_summary)\n",
    "    else:\n",
    "        print(\"No text extracted from the PDF. Please check the file path.\")\n",
    "#In this code  i forgotten to keep prompt parameter in input\n",
    "but its good enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e1b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d7268228b94d009a85635946596283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Available summarization types:\n",
      "1: Contract Dispute\n",
      "2: Employment Law\n",
      "3: Criminal Law\n",
      "4: Intellectual Property\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 08:54:18,695] A new study created in memory with name: no-name-54f36d3b-17e6-4634-abf6-8400151192a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing summarization parameters with 15 trials for conciseness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 08:54:30,219] Trial 0 finished with value: 846.0 and parameters: {'max_new_tokens': 164, 'num_beams': 4, 'temperature': 0.10449297595447743, 'do_sample': True, 'top_p': 0.8187036864928329}. Best is trial 0 with value: 846.0.\n",
      "[I 2025-03-02 08:54:42,160] Trial 1 finished with value: 911.0 and parameters: {'max_new_tokens': 177, 'num_beams': 4, 'temperature': 0.18076788567526092, 'do_sample': True, 'top_p': 0.8668709012360236}. Best is trial 1 with value: 911.0.\n",
      "[I 2025-03-02 08:54:58,481] Trial 2 finished with value: 1025.0 and parameters: {'max_new_tokens': 200, 'num_beams': 6, 'temperature': 0.20449127588870547, 'do_sample': True, 'top_p': 0.8034672831956983}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:55:13,953] Trial 3 finished with value: 1003.0 and parameters: {'max_new_tokens': 195, 'num_beams': 6, 'temperature': 0.2522550576414827, 'do_sample': True, 'top_p': 0.8406816741340758}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:55:26,075] Trial 4 finished with value: 883.0 and parameters: {'max_new_tokens': 168, 'num_beams': 5, 'temperature': 0.12311188616441482, 'do_sample': True, 'top_p': 0.9113741171223525}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:55:32,812] Trial 5 finished with value: 558.0 and parameters: {'max_new_tokens': 101, 'num_beams': 4, 'temperature': 0.13647335763909813, 'do_sample': True, 'top_p': 0.8077260393709753}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:55:41,692] Trial 6 finished with value: 570.0 and parameters: {'max_new_tokens': 108, 'num_beams': 6, 'temperature': 0.19087113540215772, 'do_sample': True, 'top_p': 0.8911459656690145}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:55:53,688] Trial 7 finished with value: 795.0 and parameters: {'max_new_tokens': 151, 'num_beams': 6, 'temperature': 0.15775463740738635, 'do_sample': True, 'top_p': 0.846007839538557}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:56:02,209] Trial 8 finished with value: 653.0 and parameters: {'max_new_tokens': 121, 'num_beams': 5, 'temperature': 0.2623978748686202, 'do_sample': True, 'top_p': 0.8522045779404732}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:56:14,527] Trial 9 finished with value: 954.0 and parameters: {'max_new_tokens': 185, 'num_beams': 4, 'temperature': 0.21442273767317252, 'do_sample': True, 'top_p': 0.8955840639660082}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:56:25,204] Trial 10 finished with value: 732.0 and parameters: {'max_new_tokens': 139, 'num_beams': 6, 'temperature': 0.2943711979646165, 'do_sample': True, 'top_p': 0.935393017661118}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:56:41,446] Trial 11 finished with value: 1022.0 and parameters: {'max_new_tokens': 199, 'num_beams': 6, 'temperature': 0.23008327463985545, 'do_sample': True, 'top_p': 0.8242836650055024}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:56:55,023] Trial 12 finished with value: 1025.0 and parameters: {'max_new_tokens': 197, 'num_beams': 5, 'temperature': 0.2233914855678041, 'do_sample': True, 'top_p': 0.8057414866395398}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:57:08,216] Trial 13 finished with value: 961.0 and parameters: {'max_new_tokens': 184, 'num_beams': 5, 'temperature': 0.2318820350496207, 'do_sample': True, 'top_p': 0.8007434960652496}. Best is trial 2 with value: 1025.0.\n",
      "[I 2025-03-02 08:57:22,536] Trial 14 finished with value: 1049.0 and parameters: {'max_new_tokens': 200, 'num_beams': 5, 'temperature': 0.16710260016354653, 'do_sample': True, 'top_p': 0.8285242131289229}. Best is trial 14 with value: 1049.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL CONCISE SUMMARY ===\n",
      "\n",
      "The defendant was charged with first-degree murder, aggravated assault, and burglary. The prosecution argued that the defendant was the perpetrator of the crime. The defense argued that there was no evidence linking the defendant to the crime and that the evidence was unreliable. The court found that the prosecution had failed to prove its case beyond a reasonable doubt and therefore acquitted the defendant of all charges. The case was dismissed. The facts of the case are as follows: The defendant was a freelance graphic designer and the victim was a high school teacher. They had been in a relationship for two years and had a child together. On the night of the incident, the defendant called the victim and asked her to meet him at his apartment. The victim arrived at the apartment and was met by the defendant, who was armed with a knife. He stabbed her multiple times and then fled the scene. The police were called to the scene and found the victim lying on the floor of the apartment. She was bleeding profusely and was pronounced dead\n"
     ]
    }
   ],
   "source": [
    "#final_run\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import optuna\n",
    "import re\n",
    "import os\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes illegal characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts and processes text from a PDF file.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' does not exist.\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        doc.close()\n",
    "        return sanitize_text(text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTION ==========\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"Loads tokenizer and model efficiently.\"\"\"\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")  # Move to GPU if available\n",
    "\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# ========== LEGAL PROMPT SELECTION ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates structured prompts for concise summarization.\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"contract_dispute\": \"\"\"\n",
    "Summarize this contract dispute concisely:\n",
    "1. Parties involved\n",
    "2. Nature of the contract and alleged breach\n",
    "3. Key legal arguments from both sides\n",
    "4. Case status or resolution\n",
    "\"\"\",\n",
    "        \"employment_law\": \"\"\"\n",
    "Summarize this employment law case concisely:\n",
    "1. Employee/employer details\n",
    "2. Nature of the dispute\n",
    "3. Key claims by the employee\n",
    "4. Key defenses by the employer\n",
    "5. Case status\n",
    "\"\"\",\n",
    "        \"criminal_law\": \"\"\"\n",
    "Summarize this criminal case concisely:\n",
    "1. Defendant details and charges\n",
    "2. Key evidence presented\n",
    "3. Prosecution's main arguments\n",
    "4. Defense counterarguments\n",
    "5. Case status\n",
    "\"\"\",\n",
    "        \"intellectual_property\": \"\"\"\n",
    "Summarize this intellectual property case concisely:\n",
    "1. Parties involved\n",
    "2. Nature of alleged infringement\n",
    "3. Key legal arguments from both sides\n",
    "4. Case status or ruling\n",
    "\"\"\",\n",
    "        \"default_prompt\": \"\"\"\n",
    "Summarize this legal case concisely:\n",
    "1. Key parties involved\n",
    "2. Nature of the dispute\n",
    "3. Main arguments from both sides\n",
    "4. Current status or resolution\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    return prompts.get(case_type.lower(), prompts[\"default_prompt\"])\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTION ==========\n",
    "\n",
    "def generate_concise_summary(input_text, model, tokenizer, hyperparams, case_type):\n",
    "    \"\"\"Generates a structured, concise summary with optimized hyperparameters.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\\n\"\n",
    "\n",
    "    model_inputs = tokenizer(input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=hyperparams.get(\"max_new_tokens\", 150),  # Enforced conciseness\n",
    "            num_beams=hyperparams.get(\"num_beams\", 5),  \n",
    "            temperature=hyperparams.get(\"temperature\", 0.2),\n",
    "            do_sample=hyperparams.get(\"do_sample\", True),\n",
    "            top_p=hyperparams.get(\"top_p\", 0.85),\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return sanitize_text(full_output.split(\"### Summary:\")[-1].strip())\n",
    "\n",
    "# ========== OBJECTIVE FUNCTION FOR OPTUNA ==========\n",
    "\n",
    "def objective(trial, input_text, model, tokenizer, case_type):\n",
    "    \"\"\"Objective function for hyperparameter tuning using Optuna.\"\"\"\n",
    "    summary = generate_concise_summary(input_text, model, tokenizer, {\n",
    "        \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 100, 200),  # Concise summary\n",
    "        \"num_beams\": trial.suggest_int(\"num_beams\", 4, 6),\n",
    "        \"temperature\": trial.suggest_float(\"temperature\", 0.1, 0.3),\n",
    "        \"do_sample\": trial.suggest_categorical(\"do_sample\", [True]),\n",
    "        \"top_p\": trial.suggest_float(\"top_p\", 0.8, 0.95)\n",
    "    }, case_type)\n",
    "\n",
    "    return len(summary)  # Placeholder scoring (use evaluation metric if needed)\n",
    "\n",
    "# ========== MAIN EXECUTION BLOCK ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"coderop12/Empowering_Legal_Summarization\"\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Case type selection\n",
    "    case_types = {\n",
    "        1: \"contract_dispute\",\n",
    "        2: \"employment_law\",\n",
    "        3: \"criminal_law\",\n",
    "        4: \"intellectual_property\"\n",
    "    }\n",
    "\n",
    "    print(\"Available summarization types:\")\n",
    "    for key, value in case_types.items():\n",
    "        print(f\"{key}: {value.replace('_', ' ').title()}\")\n",
    "\n",
    "    choice = int(input(\"Enter the number corresponding to the type of summarization you want: \"))\n",
    "    case_type = case_types.get(choice, \"default_prompt\")\n",
    "\n",
    "    # PDF File Input\n",
    "    pdf_path = input(\"Enter the path to the PDF file: \")\n",
    "    input_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if input_text:\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        print(\"Optimizing summarization parameters with 15 trials for conciseness...\")\n",
    "        study.optimize(lambda trial: objective(trial, input_text, model, tokenizer, case_type), n_trials=15)  \n",
    "\n",
    "        print(\"\\n=== FINAL CONCISE SUMMARY ===\\n\")\n",
    "        concise_summary = generate_concise_summary(input_text, model, tokenizer, study.best_trial.params, case_type)\n",
    "        print(concise_summary)\n",
    "    else:\n",
    "        print(\"No text extracted from the PDF. Please check the file path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ee6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b44386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUMMARY EVALUATION RESULTS ===\n",
      "              Metric   Score\n",
      "0            ROUGE-1  0.5916\n",
      "1            ROUGE-2  0.3819\n",
      "2            ROUGE-L  0.4373\n",
      "3  Cosine Similarity  0.8449\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Define reference and generated summaries\n",
    "reference_summary = \"\"\"\n",
    "The defendant was charged with first-degree murder, aggravated assault, and burglary. \n",
    "The prosecution alleged that the defendant attacked the victim, his former partner, with a knife in his apartment. \n",
    "The defense argued that no concrete evidence linked him to the crime, and the court ruled in favor of the defense, citing reasonable doubt.\n",
    "\n",
    "Case Details:\n",
    "- Defendant: Freelance graphic designer, former partner of the victim.\n",
    "- Victim: High school teacher, in a relationship with the defendant.\n",
    "- Incident Summary: On the night of the crime, the defendant called the victim to his apartment, where she was fatally stabbed.\n",
    "- Key Evidence: The prosecution presented circumstantial evidence, but the defense challenged its reliability.\n",
    "- Court Ruling: The court acquitted the defendant due to insufficient proof beyond a reasonable doubt.\n",
    "\n",
    "The case was dismissed.\n",
    "\"\"\"\n",
    "\n",
    "generated_summary = \"\"\"\n",
    "The defendant was charged with first-degree murder, aggravated assault, and burglary. \n",
    "The prosecution argued that the defendant was the perpetrator of the crime. \n",
    "The defense argued that there was no evidence linking the defendant to the crime and that the evidence was unreliable. \n",
    "The court found that the prosecution had failed to prove its case beyond a reasonable doubt and therefore acquitted the defendant of all charges. \n",
    "The case was dismissed. The facts of the case are as follows: The defendant was a freelance graphic designer and the victim was a high school teacher. \n",
    "They had been in a relationship for two years and had a child together. On the night of the incident, the defendant called the victim and asked her to meet him at his apartment. \n",
    "The victim arrived at the apartment and was met by the defendant, who was armed with a knife. He stabbed her multiple times and then fled the scene. \n",
    "The police were called to the scene and found the victim lying on the floor of the apartment. She was bleeding profusely and was pronounced dead.\n",
    "\"\"\"\n",
    "\n",
    "# Function to compute ROUGE scores\n",
    "def compute_rouge_scores(reference_summary, generated_summary):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "    \n",
    "    return {\n",
    "        \"ROUGE-1\": round(scores['rouge1'].fmeasure, 4),\n",
    "        \"ROUGE-2\": round(scores['rouge2'].fmeasure, 4),\n",
    "        \"ROUGE-L\": round(scores['rougeL'].fmeasure, 4)\n",
    "    }\n",
    "\n",
    "# Function to compute Cosine Similarity\n",
    "def compute_cosine_similarity(text1, text2):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    return round(cosine_similarity(tfidf_matrix)[0, 1], 4)\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = compute_rouge_scores(reference_summary, generated_summary)\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "cosine_score = compute_cosine_similarity(reference_summary, generated_summary)\n",
    "\n",
    "# Combine results\n",
    "evaluation_results = {**rouge_scores, \"Cosine Similarity\": cosine_score}\n",
    "\n",
    "# Display results\n",
    "df = pd.DataFrame(evaluation_results.items(), columns=[\"Metric\", \"Score\"])\n",
    "print(\"\\n=== SUMMARY EVALUATION RESULTS ===\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
