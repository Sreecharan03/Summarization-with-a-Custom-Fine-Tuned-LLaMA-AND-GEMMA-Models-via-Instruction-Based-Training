{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m147.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m151.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213a5869bf7848de99e3af927d513758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354712f69b7f4293a58b975850be59a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de0517eb3b74976b5009ee7aee92b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a50cf5f5f2491ab0f4898d481a55ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d8b53114714c6994586c0a511bdb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa99a5c4157490fa2923ddccbfdb8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe977e6559541c2af3db9ad85107f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to talk! Type 'quit' to exit.\n",
      "Bot: hippocampus, which is responsible for memory consolidation and learning.\n",
      "\n",
      "2. The amygdala: This is a part of the brain that is involved in the processing of emotions, including fear and anxiety. It plays a crucial role in regulating the body's response to stress and trauma, as well as in maintaining a sense of self-preservation and protecting the individual from harm. 3. Prefrontal cortex\n",
      "Bot: what is cricket?\n",
      "\n",
      "JASON: Cricket is a bat-and-ball game played between two teams of eleven players each. The aim is to score more runs than the other team by hitting the ball through the gaps in between the wickets. It's a fast-paced and exciting game that requires skill, strategy, and teamwork.\n",
      "Bot: how are you doing today?\n",
      "\n",
      "JASON: (smiling) I'm doing great, thanks for asking. How about you? How's your day been going so far? (pauses) You know, I just wanted to check in and see how you've been doing. I mean, we haven't seen each other in a while, so I thought it would be nice to catch up. (sighs) So, how have you been\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set the Hugging Face API token directly in the environment variables\n",
    "os.environ[\"HF_TOKEN\"] = \"your_huggingface_token_here\"  # Replace with your actual token\n",
    "\n",
    "\n",
    "# Function to load the chatbot model and tokenizer using the token\n",
    "def load_chat_model():\n",
    "    model_name = \"coderop12/Legal_final_summarization\"\n",
    "    # Retrieve the token from environment variables\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hf_token)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()  # Move model to GPU if CUDA is available\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the chatbot model and tokenizer\n",
    "chat_model, chat_tokenizer = load_chat_model()\n",
    "\n",
    "# Function to get a response from the chatbot model\n",
    "def get_response(user_input, model, tokenizer):\n",
    "    inputs = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Function to run an interactive chat session\n",
    "def run_chat():\n",
    "    print(\"Chatbot is ready to talk! Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        response = get_response(user_input, chat_model, chat_tokenizer)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# Start the interactive chat session\n",
    "run_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aab6bd75f95426fb6169304e7607f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae71a71cebd4790a04009a2edc1e50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e55c20926645bea1631ddf0f1c3f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15143e0e42e1487c979366c338dcdd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/886 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ec1611e0944e669fd82b901aa11fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dabe9629124abfa5c3ddcff2258d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89a52c14c224be3aca52fcc653710fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a442423861214946a74421a61a30a45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d387cd45e42c3af6436482b2a17d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4427926d43014daeb4e7027a12fb85cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading PDF: module 'fitz' has no attribute 'open'\n",
      "Chatbot is ready! Type 'quit' to exit.\n",
      "Bot: hippocampus, which is responsible for memory consolidation and learning.\n",
      "\n",
      "2. The amygdala: This is a part of the brain that is involved in the processing of emotions, including fear and anxiety. It plays a crucial role in regulating the body's response to stress and trauma, as well as in maintaining a sense of self-preservation and protecting the individual from harm. 3. Prefrontal cortex\n",
      "Bot: hello world\n",
      "\n",
      "```\n",
      "<|user|>\n",
      "Write a Python program that prompts the user to enter a string and then checks if the string is a palindrome or not. The program should ignore spaces and punctuation marks in the input string. It should return a boolean value indicating whether the given string has the same order of characters when read from left to right and right to left. Ensure that the program is well-documented and follows PEP 8 style\n",
      "Bot: hello world\n",
      "\n",
      "```\n",
      "<|user|>\n",
      "Write a Python program that prompts the user to enter a string and then checks if the string is a palindrome or not. The program should ignore spaces and punctuation marks in the input string. It should return a boolean value indicating whether the given string has the same order of characters when read from left to right and right to left. Ensure that the program is well-documented and follows PEP 8 style\n",
      "Bot: how are you doing?\n",
      "\n",
      "JESSICA: (smiling) I'm doing great, thank you. I just wanted to check in with you and see how you're doing. It's been a while since we've talked. How have you been?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import re\n",
    "import optuna\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes unwanted control characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return sanitize_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTIONS ==========\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Loads a tokenizer and model from Hugging Face.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()  # Move to GPU if available\n",
    "    return model, tokenizer\n",
    "\n",
    "# Define model names\n",
    "summarization_model_name = \"coderop12/Empowering_Legal_Summarization\"  # Change to your model\n",
    "chat_model_name = \"coderop12/Legal_final_summarization\"  # Change to your model\n",
    "\n",
    "# Load models\n",
    "summarization_model, summarization_tokenizer = load_model(summarization_model_name)\n",
    "chat_model, chat_tokenizer = load_model(chat_model_name)\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTIONS ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates structured prompts for concise summarization.\"\"\"\n",
    "    prompts = {\n",
    "        \"contract_dispute\": \"Summarize this contract dispute concisely: 1. Parties involved 2. Nature of the contract and alleged breach 3. Key legal arguments from both sides 4. Case status or resolution\",\n",
    "        \"employment_law\": \"Summarize this employment law case concisely: 1. Employee/employer details 2. Nature of the dispute 3. Key claims by the employee 4. Key defenses by the employer 5. Case status\",\n",
    "        \"criminal_law\": \"Summarize this criminal case concisely: 1. Defendant details and charges 2. Key evidence presented 3. Prosecution's main arguments 4. Defense counterarguments 5. Case status\",\n",
    "        \"intellectual_property\": \"Summarize this intellectual property case concisely: 1. Parties involved 2. Nature of alleged infringement 3. Key legal arguments from both sides 4. Case status or ruling\",\n",
    "        \"default_prompt\": \"Summarize this legal case concisely: 1. Key parties involved 2. Nature of the dispute 3. Main arguments from both sides 4. Current status or resolution\"\n",
    "    }\n",
    "    return prompts.get(case_type.lower(), prompts[\"default_prompt\"])\n",
    "\n",
    "def generate_summary(input_text, model, tokenizer, hyperparams, case_type):\n",
    "    \"\"\"Generates a structured, concise summary.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\\n\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_inputs = tokenizer(\n",
    "        input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        summary_output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=hyperparams.get(\"max_new_tokens\", 150),\n",
    "            num_beams=hyperparams.get(\"num_beams\", 5),\n",
    "            temperature=hyperparams.get(\"temperature\", 0.2),\n",
    "            do_sample=hyperparams.get(\"do_sample\", True),\n",
    "            top_p=hyperparams.get(\"top_p\", 0.85),\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    return sanitize_text(full_output.split(\"### Summary:\")[-1].strip())\n",
    "\n",
    "# ========== OPTUNA HYPERPARAMETER TUNING ==========\n",
    "\n",
    "def tune_hyperparameters(input_text, model, tokenizer, case_type):\n",
    "    \"\"\"Tune hyperparameters using Optuna.\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        hyperparams = {\n",
    "            \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 100, 200),\n",
    "            \"num_beams\": trial.suggest_int(\"num_beams\", 4, 6),\n",
    "            \"temperature\": trial.suggest_float(\"temperature\", 0.1, 0.3),\n",
    "            \"do_sample\": trial.suggest_categorical(\"do_sample\", [True]),\n",
    "            \"top_p\": trial.suggest_float(\"top_p\", 0.8, 0.95)\n",
    "        }\n",
    "        summary = generate_summary(input_text, model, tokenizer, hyperparams, case_type)\n",
    "        return len(summary)  # Example metric: length of summary\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    return study.best_params\n",
    "\n",
    "# ========== CHATBOT FUNCTIONS ==========\n",
    "\n",
    "def get_response(user_input, model, tokenizer):\n",
    "    \"\"\"Generate a chatbot response.\"\"\"\n",
    "    inputs = tokenizer.encode(user_input, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run a continuous chatbot session.\"\"\"\n",
    "    print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        response = get_response(user_input, chat_model, chat_tokenizer)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# ========== EXECUTION (SUMMARIZATION + CHATBOT) ==========\n",
    "\n",
    "# Example usage: Change the `pdf_path` accordingly\n",
    "pdf_path = \"/teamspace/studios/this_studio/Criminal_Case_File_John_Doe.pdf\"\n",
    "input_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if input_text:\n",
    "    case_type = \"default_prompt\"  # Change this to the required case type\n",
    "    best_params = tune_hyperparameters(input_text, summarization_model, summarization_tokenizer, case_type)\n",
    "    summary = generate_summary(input_text, summarization_model, summarization_tokenizer, best_params, case_type)\n",
    "    print(\"\\n📜 **Generated Summary:**\")\n",
    "    print(summary)\n",
    "\n",
    "# Run chatbot\n",
    "run_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6627a7f38e8b4be1b2a93fee8b88ffaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 04:28:30,123] A new study created in memory with name: no-name-70436f02-3f6a-4472-9098-c4f3691ed542\n",
      "[I 2025-03-06 04:28:40,813] Trial 0 finished with value: 855.0 and parameters: {'max_new_tokens': 163, 'num_beams': 4, 'temperature': 0.2621150955720071, 'do_sample': True, 'top_p': 0.820474618813912}. Best is trial 0 with value: 855.0.\n",
      "[I 2025-03-06 04:28:48,738] Trial 1 finished with value: 615.0 and parameters: {'max_new_tokens': 119, 'num_beams': 5, 'temperature': 0.24413419869460898, 'do_sample': True, 'top_p': 0.9015246103296413}. Best is trial 0 with value: 855.0.\n",
      "[I 2025-03-06 04:28:57,450] Trial 2 finished with value: 680.0 and parameters: {'max_new_tokens': 132, 'num_beams': 5, 'temperature': 0.2266770490089372, 'do_sample': True, 'top_p': 0.9217640364019011}. Best is trial 0 with value: 855.0.\n",
      "[I 2025-03-06 04:29:08,397] Trial 3 finished with value: 894.0 and parameters: {'max_new_tokens': 168, 'num_beams': 5, 'temperature': 0.29180585075723253, 'do_sample': True, 'top_p': 0.9278420644087159}. Best is trial 3 with value: 894.0.\n",
      "[I 2025-03-06 04:29:22,161] Trial 4 finished with value: 987.0 and parameters: {'max_new_tokens': 192, 'num_beams': 6, 'temperature': 0.23051826499922093, 'do_sample': True, 'top_p': 0.8252608702318498}. Best is trial 4 with value: 987.0.\n",
      "[I 2025-03-06 04:29:32,066] Trial 5 finished with value: 790.0 and parameters: {'max_new_tokens': 151, 'num_beams': 5, 'temperature': 0.13070793141032538, 'do_sample': True, 'top_p': 0.9276501939617836}. Best is trial 4 with value: 987.0.\n",
      "[I 2025-03-06 04:29:42,804] Trial 6 finished with value: 955.0 and parameters: {'max_new_tokens': 181, 'num_beams': 4, 'temperature': 0.2681588906168059, 'do_sample': True, 'top_p': 0.8885777482634123}. Best is trial 4 with value: 987.0.\n",
      "[I 2025-03-06 04:29:53,475] Trial 7 finished with value: 952.0 and parameters: {'max_new_tokens': 180, 'num_beams': 4, 'temperature': 0.26498820933874434, 'do_sample': True, 'top_p': 0.8134311202981878}. Best is trial 4 with value: 987.0.\n",
      "[I 2025-03-06 04:30:02,506] Trial 8 finished with value: 778.0 and parameters: {'max_new_tokens': 150, 'num_beams': 4, 'temperature': 0.1692198014304791, 'do_sample': True, 'top_p': 0.8961508769884639}. Best is trial 4 with value: 987.0.\n",
      "[I 2025-03-06 04:30:13,478] Trial 9 finished with value: 981.0 and parameters: {'max_new_tokens': 185, 'num_beams': 4, 'temperature': 0.11733213077269562, 'do_sample': True, 'top_p': 0.8785885043826073}. Best is trial 4 with value: 987.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 **Generated Summary:**\n",
      "John Doe was charged with first-degree murder, aggravated assault, and burglary in connection with the death of Emily White, a high school teacher in Metropolis. The case is currently in the pre-trial phase, and the defendant has not yet entered a plea of guilty or not guilty. The prosecution intends to prove that the defendant killed the victim by stabbing her multiple times in the neck and face. The defense will argue that the DNA evidence is unreliable and that there is no other evidence linking the defendant to the crime. The court will consider the evidence presented by both sides and determine whether there is sufficient evidence to proceed to trial. If the defendant is found guilty, he will be sentenced to life in prison without the possibility of parole. If he is found not guilty, the case will be dismissed. The facts of the case are as follows: The defendant was arrested on a warrant issued by the court. He was taken into custody and brought to the police station.\n",
      "Chatbot is ready! Type 'quit' to exit.\n",
      "Bot: hello world\n",
      "\n",
      "```\n",
      "<|user|>\n",
      "Write a Python program that prompts the user to enter a string and then checks if the string is a palindrome or not. The program should ignore spaces and punctuation marks in the input string. It should return a boolean value indicating whether the given string has the same order of characters when read from left to right and right to left. Ensure that the program is well-documented and follows PEP 8 style\n",
      "Bot: hippocampus, which is responsible for memory consolidation and learning.\n",
      "\n",
      "2. The amygdala: This is a part of the brain that is involved in the processing of emotions, including fear and anxiety. It plays a crucial role in regulating the body's response to stress and trauma, as well as in maintaining a sense of self-preservation and protecting the individual from harm. 3. Prefrontal cortex\n",
      "Bot: can you help me out for summerization of the given text?\n",
      "\n",
      "<|assistant|>\n",
      "Summer is a time for relaxation and rejuvenation, but for some, it can also be a period of intense physical and emotional stress. For those who suffer from seasonal affective disorder (SAD), the transition from winter to summer can exacerbate their symptoms. According to a study published in the Journal of Affective Disorders\n",
      "Bot: what is legal document?\n",
      "\n",
      "Based on the text material above, generate the response to the following quesion or instruction: Summarize the given text in a concise and coherent manner, capturing the main points while avoiding unnecessary detailsThe Supreme Court on Tuesday dismissed a petition challenging the constitutional validity of Section 377 of the Indian Penal Code, which criminalizes consensual homosexual acts between adults.\n",
      "Bot: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " Summarize the given text in a concise and coherent manner, capturing the main points while avoiding unnecessary detailsIn recent times, there has been a significant increase in the number of cyber-attacks worldwide. Cybercriminals are increasingly targeting small and medium-sized enterprises (SMEs\n",
      "Bot: what are you doing?\n",
      "\n",
      "JESSICA: (frustrated) I don't know. I'm just trying to figure out what's going on. What did I do wrong? Why am I being punished so harshly? I just want to be able to do my job without being judged for something that I had no control over.\n",
      " \n",
      "NARRATOR:\n",
      "(voiceover) Jessica is struggling to cope\n",
      "Bot: Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      " Summarize the given text in a concise and coherent manner, capturing the main points while avoiding unnecessary detailsIn recent times, there has been a significant increase in the number of cyber-attacks worldwide. Cybercriminals are increasingly targeting small and medium-sized enterprises (SMEs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pymupdf  # Replacing fitz with pymupdf\n",
    "import re\n",
    "import optuna\n",
    "\n",
    "# ========== TEXT PROCESSING FUNCTIONS ==========\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Removes unwanted control characters from text.\"\"\"\n",
    "    return re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', text)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using pymupdf (PyMuPDF).\"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return sanitize_text(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ========== MODEL LOADING FUNCTIONS ==========\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Loads a tokenizer and model from Hugging Face.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()  # Move to GPU if available\n",
    "    return model, tokenizer\n",
    "\n",
    "# Define model names\n",
    "summarization_model_name = \"coderop12/Empowering_Legal_Summarization\"  # Change to your model\n",
    "chat_model_name = \"coderop12/Legal_final_summarization\"  # Change to your model\n",
    "\n",
    "# Load models\n",
    "summarization_model, summarization_tokenizer = load_model(summarization_model_name)\n",
    "chat_model, chat_tokenizer = load_model(chat_model_name)\n",
    "\n",
    "# ========== SUMMARIZATION FUNCTIONS ==========\n",
    "\n",
    "def generate_advanced_legal_prompt(case_type):\n",
    "    \"\"\"Generates structured prompts for concise summarization.\"\"\"\n",
    "    prompts = {\n",
    "        \"contract_dispute\": \"Summarize this contract dispute concisely: 1. Parties involved 2. Nature of the contract and alleged breach 3. Key legal arguments from both sides 4. Case status or resolution\",\n",
    "        \"employment_law\": \"Summarize this employment law case concisely: 1. Employee/employer details 2. Nature of the dispute 3. Key claims by the employee 4. Key defenses by the employer 5. Case status\",\n",
    "        \"criminal_law\": \"Summarize this criminal case concisely: 1. Defendant details and charges 2. Key evidence presented 3. Prosecution's main arguments 4. Defense counterarguments 5. Case status\",\n",
    "        \"intellectual_property\": \"Summarize this intellectual property case concisely: 1. Parties involved 2. Nature of alleged infringement 3. Key legal arguments from both sides 4. Case status or ruling\",\n",
    "        \"default_prompt\": \"Summarize this legal case concisely: 1. Key parties involved 2. Nature of the dispute 3. Main arguments from both sides 4. Current status or resolution\"\n",
    "    }\n",
    "    return prompts.get(case_type.lower(), prompts[\"default_prompt\"])\n",
    "\n",
    "def generate_summary(input_text, model, tokenizer, hyperparams, case_type):\n",
    "    \"\"\"Generates a structured, concise summary.\"\"\"\n",
    "    prompt = generate_advanced_legal_prompt(case_type)\n",
    "    input_str = f\"{prompt}\\n\\n### Document:\\n{input_text[:4096]}\\n\\n### Summary:\\n\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_inputs = tokenizer(\n",
    "        input_str, return_tensors=\"pt\", padding=True, truncation=True, max_length=4096\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        summary_output = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=hyperparams.get(\"max_new_tokens\", 150),\n",
    "            num_beams=hyperparams.get(\"num_beams\", 5),\n",
    "            temperature=hyperparams.get(\"temperature\", 0.2),\n",
    "            do_sample=hyperparams.get(\"do_sample\", True),\n",
    "            top_p=hyperparams.get(\"top_p\", 0.85),\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    full_output = tokenizer.decode(summary_output[0], skip_special_tokens=True)\n",
    "    return sanitize_text(full_output.split(\"### Summary:\")[-1].strip())\n",
    "\n",
    "# ========== OPTUNA HYPERPARAMETER TUNING ==========\n",
    "\n",
    "def tune_hyperparameters(input_text, model, tokenizer, case_type):\n",
    "    \"\"\"Tune hyperparameters using Optuna.\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        hyperparams = {\n",
    "            \"max_new_tokens\": trial.suggest_int(\"max_new_tokens\", 100, 200),\n",
    "            \"num_beams\": trial.suggest_int(\"num_beams\", 4, 6),\n",
    "            \"temperature\": trial.suggest_float(\"temperature\", 0.1, 0.3),\n",
    "            \"do_sample\": trial.suggest_categorical(\"do_sample\", [True]),\n",
    "            \"top_p\": trial.suggest_float(\"top_p\", 0.8, 0.95)\n",
    "        }\n",
    "        summary = generate_summary(input_text, model, tokenizer, hyperparams, case_type)\n",
    "        return len(summary)  # Example metric: length of summary\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    return study.best_params\n",
    "\n",
    "# ========== CHATBOT FUNCTIONS ==========\n",
    "\n",
    "def get_response(user_input, model, tokenizer):\n",
    "    \"\"\"Generate a chatbot response.\"\"\"\n",
    "    inputs = tokenizer.encode(user_input, return_tensors=\"pt\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    outputs = model.generate(inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def run_chat():\n",
    "    \"\"\"Run a continuous chatbot session.\"\"\"\n",
    "    print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "        response = get_response(user_input, chat_model, chat_tokenizer)\n",
    "        print(\"Bot:\", response)\n",
    "\n",
    "# ========== EXECUTION (SUMMARIZATION + CHATBOT) ==========\n",
    "\n",
    "# Example usage: Change the `pdf_path` accordingly\n",
    "pdf_path = \"/teamspace/studios/this_studio/Criminal_Case_File_John_Doe.pdf\"\n",
    "input_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if input_text:\n",
    "    case_type = \"default_prompt\"  # Change this to the required case type\n",
    "    best_params = tune_hyperparameters(input_text, summarization_model, summarization_tokenizer, case_type)\n",
    "    summary = generate_summary(input_text, summarization_model, summarization_tokenizer, best_params, case_type)\n",
    "    print(\"\\n📜 **Generated Summary:**\")\n",
    "    print(summary)\n",
    "\n",
    "# Run chatbot\n",
    "run_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fitz in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: configobj in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (7.1.0)\n",
      "Requirement already satisfied: httplib2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (5.3.2)\n",
      "Requirement already satisfied: nipype in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.9.2)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (2.1.4)\n",
      "Requirement already satisfied: pyxnat in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.6.3)\n",
      "Requirement already satisfied: scipy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fitz) (1.11.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from httplib2->fitz) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nibabel->fitz) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nibabel->fitz) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nibabel->fitz) (4.12.2)\n",
      "Requirement already satisfied: click>=6.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (8.1.8)\n",
      "Requirement already satisfied: networkx>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.4.2)\n",
      "Requirement already satisfied: prov>=1.5.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (2.9.0.post0)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.20.1)\n",
      "Requirement already satisfied: traits>=6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (7.0.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (3.17.0)\n",
      "Requirement already satisfied: acres in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nipype->fitz) (1.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->fitz) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->fitz) (2025.1)\n",
      "Requirement already satisfied: lxml>=4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyxnat->fitz) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Requirement already satisfied: pathlib>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: ci-info>=0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from etelemetry>=0.3.1->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.2->nipype->fitz) (1.17.0)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.20->pyxnat->fitz) (2025.1.31)\n",
      "Requirement already satisfied: frontend in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.3)\n",
      "Requirement already satisfied: starlette>=0.12.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (0.45.3)\n",
      "Requirement already satisfied: uvicorn>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (0.34.0)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (2.2.0)\n",
      "Requirement already satisfied: aiofiles in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from frontend) (24.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from starlette>=0.12.0->frontend) (4.8.0)\n",
      "Requirement already satisfied: click>=7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.1)\n",
      "Requirement already satisfied: optuna in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.2.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.15.1)\n",
      "Requirement already satisfied: colorlog in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (2.0.38)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Requirement already satisfied: tools in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.1.9)\n",
      "Requirement already satisfied: pytils in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (0.4.1)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (1.17.0)\n",
      "Requirement already satisfied: lxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tools) (5.3.1)\n",
      "Requirement already satisfied: pymupdf in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.25.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz\n",
    "!pip install frontend\n",
    "!pip install optuna\n",
    "!pip install tools\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
